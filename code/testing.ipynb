{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install tf-keras\n",
    "# %pip install sentencepiece\n",
    "# %pip install tensorflow_text\n",
    "# %pip install accelerate\n",
    "# %pip install tensorboard\n",
    "# %pip install tensorflow-intel\n",
    "# %pip install --upgrade protobuf\n",
    "# %pip uninstall protobuf keras tensorboard\n",
    "# %pip uninstall tensorflow tensorflow-intel tensorflow-gpu\n",
    "# %pip install protobuf==3.20.3\n",
    "%pip install keras==3.5.0\n",
    "%pip install tensorboard==2.18.0\n",
    "%pip install --upgrade onnx\n",
    "%pip install tensorflow tensorflow-intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflo2w==2.10.0\n",
    "%pip install transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degree</th>\n",
       "      <th>Role</th>\n",
       "      <th>Section</th>\n",
       "      <th>Proficiency Level</th>\n",
       "      <th>Question</th>\n",
       "      <th>Options</th>\n",
       "      <th>Correct Answer</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B.Tech in Computer Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Computational Skills</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>What is the primary purpose of the pandas grou...</td>\n",
       "      <td>['To sort data', 'To split data into groups', ...</td>\n",
       "      <td>To split data into groups</td>\n",
       "      <td>The groupby() function splits the data into gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B.Tech in Computer Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Core Programming</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Which time complexity represents binary search?</td>\n",
       "      <td>['O(n)', 'O(log n)', 'O(n log n)', 'O(1)']</td>\n",
       "      <td>O(log n)</td>\n",
       "      <td>Binary search repeatedly divides the search sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B.Tech in Computer Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analysis</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>In a dataset with outliers which visualization...</td>\n",
       "      <td>['Simple line plot', 'Box plot with whiskers',...</td>\n",
       "      <td>Box plot with whiskers</td>\n",
       "      <td>Box plots show median, quartiles, and outliers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B.Sc. in Mathematics</td>\n",
       "      <td>Risk Analyst</td>\n",
       "      <td>Core Mathematical Subjects</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>What is the variance of a constant?</td>\n",
       "      <td>['1', 'The constant value', '0', 'Undefined']</td>\n",
       "      <td>0</td>\n",
       "      <td>The variance measures spread around the mean. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B.Sc. in Mathematics</td>\n",
       "      <td>Risk Analyst</td>\n",
       "      <td>Applied Mathematics</td>\n",
       "      <td>Medium</td>\n",
       "      <td>In Value at Risk (VaR) calculation what confid...</td>\n",
       "      <td>['90%', '95%', '99%', '99.9%']</td>\n",
       "      <td>99%</td>\n",
       "      <td>99% is the standard confidence level for VaR i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Degree          Role                     Section  \\\n",
       "0  B.Tech in Computer Science  Data Analyst        Computational Skills   \n",
       "1  B.Tech in Computer Science  Data Analyst            Core Programming   \n",
       "2  B.Tech in Computer Science  Data Analyst               Data Analysis   \n",
       "3        B.Sc. in Mathematics  Risk Analyst  Core Mathematical Subjects   \n",
       "4        B.Sc. in Mathematics  Risk Analyst         Applied Mathematics   \n",
       "\n",
       "  Proficiency Level                                           Question  \\\n",
       "0          Beginner  What is the primary purpose of the pandas grou...   \n",
       "1            Medium    Which time complexity represents binary search?   \n",
       "2          Advanced  In a dataset with outliers which visualization...   \n",
       "3          Beginner                What is the variance of a constant?   \n",
       "4            Medium  In Value at Risk (VaR) calculation what confid...   \n",
       "\n",
       "                                             Options  \\\n",
       "0  ['To sort data', 'To split data into groups', ...   \n",
       "1         ['O(n)', 'O(log n)', 'O(n log n)', 'O(1)']   \n",
       "2  ['Simple line plot', 'Box plot with whiskers',...   \n",
       "3      ['1', 'The constant value', '0', 'Undefined']   \n",
       "4                     ['90%', '95%', '99%', '99.9%']   \n",
       "\n",
       "              Correct Answer  \\\n",
       "0  To split data into groups   \n",
       "1                   O(log n)   \n",
       "2     Box plot with whiskers   \n",
       "3                          0   \n",
       "4                        99%   \n",
       "\n",
       "                                         Explanation  \n",
       "0  The groupby() function splits the data into gr...  \n",
       "1  Binary search repeatedly divides the search sp...  \n",
       "2  Box plots show median, quartiles, and outliers...  \n",
       "3  The variance measures spread around the mean. ...  \n",
       "4  99% is the standard confidence level for VaR i...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "input_file = '../placement-questions-excel.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for fine-tuning\n",
    "questions = data['Question'].tolist()\n",
    "sections = data['Section'].tolist()  # Assuming a 'Section' column exists to classify questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the primary purpose of the pandas groupby() function in Python?',\n",
       " 'Which time complexity represents binary search?',\n",
       " 'In a dataset with outliers which visualization technique would be most appropriate for understanding the distribution?',\n",
       " 'What is the variance of a constant?',\n",
       " 'In Value at Risk (VaR) calculation what confidence level is typically used in financial risk management?',\n",
       " 'Which statistical test would you use to compare the means of three or more independent groups?',\n",
       " 'What is the primary difference between simple and compound interest?',\n",
       " 'Which investment strategy typically provides the highest potential return over a long-term period?',\n",
       " 'How does duration measure bond price sensitivity to interest rate changes?',\n",
       " 'Which plot type is most suitable for showing the relationship between two continuous variables?',\n",
       " 'What is the difference between INNER JOIN and LEFT JOIN?',\n",
       " 'What is the time complexity of inserting an element into a Python list at the beginning?',\n",
       " 'Which evaluation metric is most appropriate for an imbalanced classification problem?',\n",
       " 'What does a p-value of 0.05 indicate?',\n",
       " 'In a normal distribution what percentage of data falls within two standard deviations?',\n",
       " 'How does eigenvalue decomposition help in Principal Component Analysis?',\n",
       " 'What is the key assumption of the Black-Scholes option pricing model?',\n",
       " 'What is autocorrelation in time series data?',\n",
       " 'Which distribution is commonly used to model the number of insurance claims in a time period?',\n",
       " 'What is the main principle of Modern Portfolio Theory?',\n",
       " 'Which retirement account typically offers tax-deferred growth?',\n",
       " 'How does the Phillips Curve relate to monetary policy decisions?',\n",
       " 'What is the beta coefficient in the Capital Asset Pricing Model?',\n",
       " 'What is dollar-cost averaging?',\n",
       " 'What is the best approach for handling missing numerical values in a dataset?',\n",
       " 'Which technique is most appropriate for handling categorical variables with high cardinality?',\n",
       " 'How can you optimize a pandas operation on a large dataset?',\n",
       " 'What is data anonymization?',\n",
       " 'What is the key characteristic of a Markov process?',\n",
       " \"How does Jensen's inequality apply to option pricing?\",\n",
       " 'What is the purpose of confidence intervals?',\n",
       " 'Which method is most suitable for solving non-linear optimization problems with constraints?',\n",
       " 'What is tax-loss harvesting?',\n",
       " 'When does a bypass trust become most beneficial?',\n",
       " 'What is the 4% rule in retirement planning?',\n",
       " 'How does duration gap analysis help in asset-liability management?',\n",
       " 'What is the primary advantage of using a columnar database?',\n",
       " 'What is data lineage?',\n",
       " 'Why is cross-validation preferred over a single train-test split?',\n",
       " 'What is the key limitation of Value at Risk (VaR)?',\n",
       " 'How does correlation affect portfolio diversification?',\n",
       " 'What is seasonality in time series?',\n",
       " 'How does anchoring bias affect investment decisions?',\n",
       " 'What distinguishes ETFs from mutual funds?',\n",
       " 'What is the primary purpose of an Investment Policy Statement?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'target'],\n",
       "    num_rows: 180\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataset with prompts and targets\n",
    "def prepare_data(questions, sections):\n",
    "    data_dict = {'prompt': [], 'target': []}\n",
    "    for question, section in zip(questions, sections):\n",
    "        context = f\"Generate questions for the section: {section}. Base question: {question}\"\n",
    "        for i in range(4):  # Generate 4x questions for each input question\n",
    "            data_dict['prompt'].append(context)\n",
    "            data_dict['target'].append(\"<new_question>\")  # Placeholder for new questions during training\n",
    "    return data_dict\n",
    "\n",
    "# Prepare the dataset for Hugging Face\n",
    "prepared_data = prepare_data(questions, sections)\n",
    "hf_dataset = Dataset.from_dict(prepared_data)\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the T5 tokenizer and model\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 180/180 [00:00<00:00, 2383.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 180\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['prompt'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples['target'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, prompt. If target, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 180\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 135\n",
      "  Number of trainable parameters = 222903552\n",
      "  7%|▋         | 10/135 [01:23<16:06,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.9154, 'learning_rate': 4.62962962962963e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 20/135 [02:41<14:31,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7477, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 30/135 [03:58<13:18,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4491, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 40/135 [05:19<12:59,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2329, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 45/135 [06:01<12:21,  8.24s/it]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, prompt. If target, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 180\n",
      "  Batch size = 8\n",
      "                                                \n",
      " 33%|███▎      | 45/135 [07:37<12:21,  8.24s/it]Saving model checkpoint to ./t5-fine-tuned\\checkpoint-45\n",
      "Configuration saved in ./t5-fine-tuned\\checkpoint-45\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09622772783041, 'eval_runtime': 96.332, 'eval_samples_per_second': 1.869, 'eval_steps_per_second': 0.239, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./t5-fine-tuned\\checkpoint-45\\pytorch_model.bin\n",
      "tokenizer config file saved in ./t5-fine-tuned\\checkpoint-45\\tokenizer_config.json\n",
      "Special tokens file saved in ./t5-fine-tuned\\checkpoint-45\\special_tokens_map.json\n",
      " 37%|███▋      | 50/135 [08:55<25:08, 17.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1276, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 60/135 [10:12<09:49,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0856, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 70/135 [11:28<08:10,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0654, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 80/135 [12:45<06:53,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0345, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 90/135 [14:01<05:38,  7.53s/it]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, prompt. If target, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 180\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0248, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 67%|██████▋   | 90/135 [15:36<05:38,  7.53s/it]Saving model checkpoint to ./t5-fine-tuned\\checkpoint-90\n",
      "Configuration saved in ./t5-fine-tuned\\checkpoint-90\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00019901295308955014, 'eval_runtime': 95.5153, 'eval_samples_per_second': 1.885, 'eval_steps_per_second': 0.241, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./t5-fine-tuned\\checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./t5-fine-tuned\\checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./t5-fine-tuned\\checkpoint-90\\special_tokens_map.json\n",
      " 74%|███████▍  | 100/135 [17:48<05:11,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0176, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 110/135 [18:58<02:54,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0081, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 120/135 [20:08<01:43,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 130/135 [21:16<00:34,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.01, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [21:53<00:00,  7.28s/it]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: target, prompt. If target, prompt are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 180\n",
      "  Batch size = 8\n",
      "                                                 \n",
      "100%|██████████| 135/135 [23:19<00:00,  7.28s/it]Saving model checkpoint to ./t5-fine-tuned\\checkpoint-135\n",
      "Configuration saved in ./t5-fine-tuned\\checkpoint-135\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00010136591299669817, 'eval_runtime': 85.8534, 'eval_samples_per_second': 2.097, 'eval_steps_per_second': 0.268, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./t5-fine-tuned\\checkpoint-135\\pytorch_model.bin\n",
      "tokenizer config file saved in ./t5-fine-tuned\\checkpoint-135\\tokenizer_config.json\n",
      "Special tokens file saved in ./t5-fine-tuned\\checkpoint-135\\special_tokens_map.json\n",
      "Deleting older checkpoint [t5-fine-tuned\\checkpoint-45] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./t5-fine-tuned\\checkpoint-135 (score: 0.00010136591299669817).\n",
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2024: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
      "100%|██████████| 135/135 [23:27<00:00, 10.42s/it]\n",
      "Saving model checkpoint to ./t5-fine-tuned\n",
      "Configuration saved in ./t5-fine-tuned\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1407.3569, 'train_samples_per_second': 0.384, 'train_steps_per_second': 0.096, 'train_loss': 1.0168263367204755, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./t5-fine-tuned\\pytorch_model.bin\n",
      "tokenizer config file saved in ./t5-fine-tuned\\tokenizer_config.json\n",
      "Special tokens file saved in ./t5-fine-tuned\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-fine-tuned\",  \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./t5-fine-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new questions based on the input\n",
    "def generate_questions(input_questions, section):\n",
    "    results = []\n",
    "    for question in input_questions:\n",
    "        context = f\"Generate questions for the section: {section}. Base question: {question}\"\n",
    "        inputs = tokenizer(context, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "        outputs = model.generate(**inputs, max_length=128, num_return_sequences=4, num_beams=4)\n",
    "        decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        results.extend(decoded_outputs)\n",
    "    return results\n",
    "\n",
    "# Example inference\n",
    "input_questions = questions[:10]  # Test with the first 10 questions\n",
    "section = \"Communication\"  # Replace with relevant section\n",
    "new_questions = generate_questions(input_questions, section)\n",
    "\n",
    "# Save the results\n",
    "output_df = pd.DataFrame({\n",
    "    \"Base Question\": input_questions * 4,  # Repeat each input question 4 times\n",
    "    \"Generated Question\": new_questions\n",
    "})\n",
    "output_df.to_csv(\"../expanded_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
